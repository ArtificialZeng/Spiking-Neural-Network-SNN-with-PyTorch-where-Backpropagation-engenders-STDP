{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiking Neural Network (SNN) with PyTorch : towards bridging the gap between deep learning and the human brain\n",
    "\n",
    "So I simply thought \"hey, what about coding a Spiking Neural Network?\" Here it is. It couldn't get that out of my head so I coded it out of curiosity. \n",
    "\n",
    "Spiking Neural Networks (SNNs) are neural networks that are closer to what happens in the brain than what people usually code when doing Machine Learning and Deep Learning. In the case of the SNN, the neurons accumulate the input activation until a threshold is reached, and when this threshold is reached, the neuron empties itself from it's activation and fire. Once empty, it should indeed take some [refractory period](https://en.wikipedia.org/wiki/Refractory_period_(physiology)) until it fires again, as it happen in the brain. So I roughly replicated this behavior here with PyTorch. I coded this without reading existing code for me to come up with a solution by myself.\n",
    "\n",
    "## How does it works?\n",
    "\n",
    "The way I define a neuron's firing method is through the following steps, where the argument `x` is an input:\n",
    "\n",
    "- Before anything, we need an empty inner state for each neuron, and for each sample in the batch. \n",
    "```python\n",
    "    self.prev_inner = torch.zeros([batch_size, self.n_hidden]).to(self.device)\n",
    "    self.prev_outer = torch.zeros([batch_size, self.n_hidden]).to(self.device)\n",
    "```\n",
    "- Then, a weight matrix multiplies the input x, which is an input from spiking neurons from below: \n",
    "```python\n",
    "    input_excitation = self.fully_connected(x)\n",
    "```\n",
    "- We then add the result to a decayed version of the information we already had at the previous time step / time tick. The `decay_multiplier` serves the purpose of slowly fading the inner activation such that we don't accumulate stimulis for too long to be able to have the neurons to rest. The `decay_multiplier` could have a value of 0.9 for example. Decay as such is also called exponential decay and yields an effect of [Exponential moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) on the most recent values seen, which also affects the gradients. In this sense, it's now really true that neurons that fire together wire together: when an input is received closer to the moment of giving an output, the gradient will be strong and learning will be able to take place. In the opposite case, a stimuli that happened too long ago will suffer from vanishing gradients since it has been exponentially decayed down. \n",
    "```python\n",
    "    inner_excitation = input_excitation + self.prev_inner * self.decay_multiplier\n",
    "```\n",
    "- Now, we compute the activation of the neurons to find their output value. We have a threshold to reach before having the neuron activating. The ReLU function may not be the most appropriate here, but let's get a prototype working fast: \n",
    "```python\n",
    "    outer_excitation = F.relu(inner_excitation - self.threshold)\n",
    "```\n",
    "- If the neuron fires, the activation of the neuron is subtracted to its inner state to reset each neuron to a reset position not to have them jammed, firing constantly. Here, I even added an extra penalty named `penalty_threshold` instead of `threshold` as above, for increasing even more the refractory time. I wasn't sure wheter the negative period in the refractory period was on the outputs of the neurons or inside the neurons, so here I exagerated it inside by subtracting a bigger value than the initial threshold to punish the neurons, although it would probably also work to have this negative part being found as an output signal rather than as an inner signal. I went with what I judged safe without digging far in research. Let's see how I subtract this just when the neuron fires to have it to have a refractory period: \n",
    "```python\n",
    "    do_penalize_gate = (outer_excitation > 0).float()\n",
    "    inner_excitation = inner_excitation - (self.penalty_threshold + outer_excitation) * do_penalize_gate\n",
    "```\n",
    "- Finally, I return the previous output, simulating a small firing delay, which is useless for now, but which may be interesting to have if the SNN I coded was ever to have Recurrent connections which would require time offsets in the connections from top layers near the outputs back into bottom layers near the input: \n",
    "```python\n",
    "    delayed_return_state = self.prev_inner\n",
    "    delayed_return_output = self.prev_outer\n",
    "    self.prev_inner = inner_excitation\n",
    "    self.prev_outer = outer_excitation\n",
    "    return delayed_return_state, delayed_return_output\n",
    "```\n",
    "\n",
    "Amazingly, it worked on the 1st try once the dimension mismatching errors were fixed. And the accuracy was about the same of the accuracy of a simple non-spiking Feedforward Neural Network with the same number of neurons. And I didn't even tuned the threshold. In the end, I realized that coding and training a Spiking Neural Network (SNN) with PyTorch was easy enough as shown above, it can be coded in an evening as such. \n",
    "\n",
    "\n",
    "\n",
    "Basically, the neurons' activation must decay through time and fire only when getting past a certain threshold. So I've gated the output of the \n",
    "\n",
    "## SNNs v.s. RNNs\n",
    "\n",
    "The SNN is NOT an RNN, despite it evolves through time too. For this SNN to be an RNN, I believe it would require some more connections such as from the outputs back into the inputs. In fact, RNNs are defined as a function of some inputs and of many neurons at the previous time step, such as:\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=$o_t&space;=&space;f(o_{t-1},&space;x_t)$\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?$o_t&space;=&space;f(o_{t-1},&space;x_t)$\" title=\"$o_t = f(o_{t-1}, x_t)$\" /></a> \n",
    "\n",
    "for example. In our case, we keep some state, but it's nothing comparable to having a connection back to other neurons in the past. \n",
    "\n",
    "## Results\n",
    "\n",
    "Cleaner-than-average code awaits below. Scroll on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torchvision.datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, device, train_set_loader, optimizer, epoch, logging_interval=100):\n",
    "    # This method is derived from: \n",
    "    # https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "    # Was licensed BSD-3-clause\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_set_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % logging_interval == 0:\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct = pred.eq(target.view_as(pred)).float().mean().item()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f} Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_set_loader.dataset),\n",
    "                100. * batch_idx / len(train_set_loader), loss.item(),\n",
    "                100. * correct))\n",
    "\n",
    "def test(model, device, test_set_loader):\n",
    "    # This method is derived from: \n",
    "    # https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "    # Was licensed BSD-3-clause\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_set_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduce=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_set_loader.dataset)\n",
    "    print(\"\")\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, \n",
    "        correct, len(test_set_loader.dataset),\n",
    "        100. * correct / len(test_set_loader.dataset)))\n",
    "    print(\"\")\n",
    "\n",
    "def download_mnist(data_path):\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "    training_set = torchvision.datasets.MNIST(data_path, train=True, transform=transformation, download=True)\n",
    "    testing_set = torchvision.datasets.MNIST(data_path, train=False, transform=transformation, download=True)\n",
    "    return training_set, testing_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "test_batch_size = 1  # TODO: fix this\n",
    "DATA_PATH = './data'\n",
    "\n",
    "training_set, testing_set = download_mnist(DATA_PATH)\n",
    "train_set_loader = torch.utils.data.DataLoader(\n",
    "    dataset=training_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "test_set_loader = torch.utils.data.DataLoader(\n",
    "    dataset=testing_set,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingNeuronLayerRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, n_inputs=28*28, n_hidden=100, decay_multiplier=0.9, threshold=2.0, penalty_threshold=2.5):\n",
    "        super(SpikingNeuronLayerRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.decay_multiplier = decay_multiplier\n",
    "        self.threshold = threshold\n",
    "        self.penalty_threshold = penalty_threshold\n",
    "        \n",
    "        self.fc = nn.Linear(n_inputs, n_hidden)\n",
    "        \n",
    "        self.init_parameters()\n",
    "        self.reset_state()\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.dim() >= 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.prev_inner = torch.zeros([self.n_hidden]).to(self.device)\n",
    "        self.prev_outer = torch.zeros([self.n_hidden]).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Call the neuron at every time step.\n",
    "        \n",
    "        x: activated_neurons_below\n",
    "        \n",
    "        return: a tuple of (state, output) for each time step. Each item in the tuple\n",
    "        are then themselves of shape (batch_size, n_hidden) and are PyTorch objects, such \n",
    "        that the whole returned would be of shape (2, batch_size, n_hidden) if casted.\n",
    "        \"\"\"\n",
    "        if self.prev_inner.dim() == 1:\n",
    "            # Adding batch_size dimension directly after doing a `self.reset_state()`:\n",
    "            batch_size = x.shape[0]\n",
    "            self.prev_inner = torch.stack(batch_size * [self.prev_inner])\n",
    "            self.prev_outer = torch.stack(batch_size * [self.prev_outer])\n",
    "        \n",
    "        # 1. Weight matrix multiplies the input x\n",
    "        input_excitation = self.fc(x)\n",
    "        \n",
    "        # 2. We add the result to a decayed version of the information we already had.\n",
    "        inner_excitation = input_excitation + self.prev_inner * self.decay_multiplier\n",
    "        \n",
    "        # 3. We compute the activation of the neuron to find its output value, \n",
    "        #    but before the activation, there is also a negative bias that refrain thing from firing too much.\n",
    "        outer_excitation = F.relu(inner_excitation - self.threshold)\n",
    "        \n",
    "        # 4. If the neuron fires, the activation of the neuron is subtracted to its inner state \n",
    "        #    (and with an extra penalty for increase refractory time), \n",
    "        #    because it discharges naturally so it shouldn't fire twice. \n",
    "        do_penalize_gate = (outer_excitation > 0).float()\n",
    "        inner_excitation = inner_excitation - (self.penalty_threshold + outer_excitation) * do_penalize_gate\n",
    "        \n",
    "        # 5. The returned value is normalized from a learnable scalar to help normalize the learning.\n",
    "        outer_excitation = outer_excitation  # TODO: code this\n",
    "        \n",
    "        # 6. Setting internal values before returning. \n",
    "        #    And the returning value is the one of the previous time step to delay \n",
    "        #    activation of 1 time step of \"processing\" time. For logits, we don't take activation.\n",
    "        delayed_return_state = self.prev_inner\n",
    "        delayed_return_output = self.prev_outer\n",
    "        self.prev_inner = inner_excitation\n",
    "        self.prev_outer = outer_excitation\n",
    "        return delayed_return_state, delayed_return_output\n",
    "\n",
    "\n",
    "class InputDataToSpikingPerceptronLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super(InputDataToSpikingPerceptronLayer, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.reset_state()\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        #     self.prev_state = torch.zeros([self.n_hidden]).to(self.device)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, is_2D=True):\n",
    "        x = x.view(x.size(0), -1)  # Flatten 2D image to 1D for FC\n",
    "        random_activation_perceptron = torch.rand(x.shape).to(self.device)\n",
    "        return random_activation_perceptron * x\n",
    "\n",
    "\n",
    "class OutputDataToSpikingPerceptronLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, average_output=True):\n",
    "        \"\"\"\n",
    "        average_output: might be needed if this is used within a regular neural net as a layer.\n",
    "        Otherwise, sum may be numerically more stable for gradients with setting average_output=False.\n",
    "        \"\"\"\n",
    "        super(OutputDataToSpikingPerceptronLayer, self).__init__()\n",
    "        if average_output:\n",
    "            self.reducer = lambda x, dim: x.sum(dim=dim)\n",
    "        else:\n",
    "            self.reducer = lambda x, dim: x.mean(dim=dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if type(x) == list:\n",
    "            x = torch.stack(x)\n",
    "        return self.reducer(x, 0)\n",
    "\n",
    "\n",
    "class SpikingNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, n_time_steps, begin_eval):\n",
    "        super(SpikingNet, self).__init__()\n",
    "        assert (0 <= begin_eval and begin_eval < n_time_steps)\n",
    "        self.device = device\n",
    "        self.n_time_steps = n_time_steps\n",
    "        self.begin_eval = begin_eval\n",
    "        \n",
    "        self.input_conversion = InputDataToSpikingPerceptronLayer(device)\n",
    "        \n",
    "        self.layer1 = SpikingNeuronLayerRNN(\n",
    "            device, n_inputs=28*28, n_hidden=100,\n",
    "            decay_multiplier=0.9, threshold=2.0, penalty_threshold=2.5\n",
    "        )\n",
    "        \n",
    "        self.layer2 = SpikingNeuronLayerRNN(\n",
    "            device, n_inputs=100, n_hidden=10,\n",
    "            decay_multiplier=0.9, threshold=2.0, penalty_threshold=2.5\n",
    "        )\n",
    "        \n",
    "        self.output_conversion = OutputDataToSpikingPerceptronLayer(average_output=False)  # Sum on outputs.\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward_through_time(self, x):\n",
    "        \"\"\"\n",
    "        This acts as a layer. Its input is non-time-related, and its output too.\n",
    "        So the time iterations happens inside, and the returned layer is thus\n",
    "        passed through global average pooling on the time axis before the return \n",
    "        such as to be able to mix this pipeline with regular backprop layers such\n",
    "        as the input data and the output data.\n",
    "        \"\"\"\n",
    "        self.input_conversion.reset_state()\n",
    "        self.layer1.reset_state()\n",
    "        self.layer2.reset_state()\n",
    "\n",
    "        out = []\n",
    "        \n",
    "        for _ in range(self.n_time_steps):\n",
    "            xi = self.input_conversion(x)\n",
    "            \n",
    "            # For layer 1, we take the regular output.\n",
    "            _, layer1_output = self.layer1(xi)\n",
    "            \n",
    "            # We take inner state of layer 2 because it's pre-activation and thus acts as out logits.\n",
    "            layer2_state, _ = self.layer2(layer1_output)\n",
    "            \n",
    "            out.append(layer2_state)\n",
    "        out = self.output_conversion(out[self.begin_eval:])\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.forward_through_time(x)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "class NonSpikingNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NonSpikingNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(28*28, 100)\n",
    "        self.layer2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x, is_2D=True):\n",
    "        x = x.view(x.size(0), -1)  # Flatten 2D image to 1D for FC\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x =        self.layer2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Spiking Neural Network (SNN)\n",
    "\n",
    "Let's use our `SpikingNet`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)] Loss: 2.377824 Accuracy: 9.40%\n",
      "Train Epoch: 1 [1000/60000 (2%)] Loss: 2.233680 Accuracy: 15.50%\n",
      "Train Epoch: 1 [2000/60000 (3%)] Loss: 2.175488 Accuracy: 14.20%\n",
      "Train Epoch: 1 [3000/60000 (5%)] Loss: 2.161426 Accuracy: 25.60%\n",
      "Train Epoch: 1 [4000/60000 (7%)] Loss: 2.348027 Accuracy: 9.00%\n",
      "Train Epoch: 1 [5000/60000 (8%)] Loss: 2.567429 Accuracy: 11.30%\n",
      "Train Epoch: 1 [6000/60000 (10%)] Loss: 1.907616 Accuracy: 29.90%\n",
      "Train Epoch: 1 [7000/60000 (12%)] Loss: 2.728153 Accuracy: 12.80%\n",
      "Train Epoch: 1 [8000/60000 (13%)] Loss: 2.211132 Accuracy: 8.60%\n",
      "Train Epoch: 1 [9000/60000 (15%)] Loss: 1.737117 Accuracy: 41.60%\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 1.716103 Accuracy: 43.30%\n",
      "Train Epoch: 1 [11000/60000 (18%)] Loss: 1.754918 Accuracy: 30.80%\n",
      "Train Epoch: 1 [12000/60000 (20%)] Loss: 2.166108 Accuracy: 37.10%\n",
      "Train Epoch: 1 [13000/60000 (22%)] Loss: 1.825984 Accuracy: 43.30%\n",
      "Train Epoch: 1 [14000/60000 (23%)] Loss: 2.246151 Accuracy: 13.30%\n",
      "Train Epoch: 1 [15000/60000 (25%)] Loss: 2.068461 Accuracy: 40.90%\n",
      "Train Epoch: 1 [16000/60000 (27%)] Loss: 2.015495 Accuracy: 29.10%\n",
      "Train Epoch: 1 [17000/60000 (28%)] Loss: 1.702660 Accuracy: 30.70%\n",
      "Train Epoch: 1 [18000/60000 (30%)] Loss: 1.387804 Accuracy: 45.60%\n",
      "Train Epoch: 1 [19000/60000 (32%)] Loss: 1.665341 Accuracy: 45.30%\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 1.383690 Accuracy: 57.40%\n",
      "Train Epoch: 1 [21000/60000 (35%)] Loss: 1.453027 Accuracy: 43.60%\n",
      "Train Epoch: 1 [22000/60000 (37%)] Loss: 1.386189 Accuracy: 54.60%\n",
      "Train Epoch: 1 [23000/60000 (38%)] Loss: 1.599594 Accuracy: 53.60%\n",
      "Train Epoch: 1 [24000/60000 (40%)] Loss: 1.747578 Accuracy: 58.70%\n",
      "Train Epoch: 1 [25000/60000 (42%)] Loss: 1.706475 Accuracy: 46.70%\n",
      "Train Epoch: 1 [26000/60000 (43%)] Loss: 1.720912 Accuracy: 43.30%\n",
      "Train Epoch: 1 [27000/60000 (45%)] Loss: 1.545690 Accuracy: 46.90%\n",
      "Train Epoch: 1 [28000/60000 (47%)] Loss: 1.342681 Accuracy: 49.80%\n",
      "Train Epoch: 1 [29000/60000 (48%)] Loss: 1.130528 Accuracy: 61.70%\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.794325 Accuracy: 76.80%\n",
      "Train Epoch: 1 [31000/60000 (52%)] Loss: 0.855687 Accuracy: 71.10%\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.705865 Accuracy: 76.60%\n",
      "Train Epoch: 1 [33000/60000 (55%)] Loss: 0.683490 Accuracy: 76.60%\n",
      "Train Epoch: 1 [34000/60000 (57%)] Loss: 0.689654 Accuracy: 79.50%\n",
      "Train Epoch: 1 [35000/60000 (58%)] Loss: 0.611533 Accuracy: 80.60%\n",
      "Train Epoch: 1 [36000/60000 (60%)] Loss: 0.574972 Accuracy: 82.80%\n",
      "Train Epoch: 1 [37000/60000 (62%)] Loss: 0.600240 Accuracy: 79.90%\n",
      "Train Epoch: 1 [38000/60000 (63%)] Loss: 0.571459 Accuracy: 82.00%\n",
      "Train Epoch: 1 [39000/60000 (65%)] Loss: 0.597598 Accuracy: 80.70%\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.647903 Accuracy: 77.20%\n",
      "Train Epoch: 1 [41000/60000 (68%)] Loss: 0.844460 Accuracy: 73.80%\n",
      "Train Epoch: 1 [42000/60000 (70%)] Loss: 0.928325 Accuracy: 69.00%\n",
      "Train Epoch: 1 [43000/60000 (72%)] Loss: 0.827311 Accuracy: 75.50%\n",
      "Train Epoch: 1 [44000/60000 (73%)] Loss: 0.767467 Accuracy: 72.90%\n",
      "Train Epoch: 1 [45000/60000 (75%)] Loss: 0.695023 Accuracy: 75.40%\n",
      "Train Epoch: 1 [46000/60000 (77%)] Loss: 0.538281 Accuracy: 84.00%\n",
      "Train Epoch: 1 [47000/60000 (78%)] Loss: 0.634588 Accuracy: 77.60%\n",
      "Train Epoch: 1 [48000/60000 (80%)] Loss: 0.594304 Accuracy: 81.10%\n",
      "Train Epoch: 1 [49000/60000 (82%)] Loss: 0.525761 Accuracy: 84.80%\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.501157 Accuracy: 85.40%\n",
      "Train Epoch: 1 [51000/60000 (85%)] Loss: 0.457713 Accuracy: 86.90%\n",
      "Train Epoch: 1 [52000/60000 (87%)] Loss: 0.469795 Accuracy: 84.40%\n",
      "Train Epoch: 1 [53000/60000 (88%)] Loss: 0.450110 Accuracy: 85.10%\n",
      "Train Epoch: 1 [54000/60000 (90%)] Loss: 0.452802 Accuracy: 85.70%\n",
      "Train Epoch: 1 [55000/60000 (92%)] Loss: 0.497998 Accuracy: 84.30%\n",
      "Train Epoch: 1 [56000/60000 (93%)] Loss: 0.473405 Accuracy: 86.50%\n",
      "Train Epoch: 1 [57000/60000 (95%)] Loss: 0.440459 Accuracy: 86.30%\n",
      "Train Epoch: 1 [58000/60000 (97%)] Loss: 0.483454 Accuracy: 85.80%\n",
      "Train Epoch: 1 [59000/60000 (98%)] Loss: 0.453323 Accuracy: 86.00%\n",
      "\n",
      "Test set: Average loss: 0.4547, Accuracy: 8555/10000 (85.55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SpikingNet(device, n_time_steps=128, begin_eval=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "\n",
    "# 1 pass on the training data:\n",
    "train(model, device, train_set_loader, optimizer, epoch, logging_interval=1)\n",
    "test(model, device, test_set_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Feedforward Neural Network (for comparison)\n",
    "\n",
    "It has the same number of layers and neurons, and also uses ReLU activation, but it's not an SNN, this one is a regular one as defined in the code above with this other class `NonSpikingNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)] Loss: 2.332794 Accuracy: 3.40%\n",
      "Train Epoch: 1 [1000/60000 (2%)] Loss: 2.304391 Accuracy: 8.70%\n",
      "Train Epoch: 1 [2000/60000 (3%)] Loss: 2.280267 Accuracy: 12.00%\n",
      "Train Epoch: 1 [3000/60000 (5%)] Loss: 2.255747 Accuracy: 14.90%\n",
      "Train Epoch: 1 [4000/60000 (7%)] Loss: 2.214018 Accuracy: 40.70%\n",
      "Train Epoch: 1 [5000/60000 (8%)] Loss: 2.187079 Accuracy: 41.70%\n",
      "Train Epoch: 1 [6000/60000 (10%)] Loss: 2.142874 Accuracy: 46.20%\n",
      "Train Epoch: 1 [7000/60000 (12%)] Loss: 2.112334 Accuracy: 46.30%\n",
      "Train Epoch: 1 [8000/60000 (13%)] Loss: 2.054627 Accuracy: 58.40%\n",
      "Train Epoch: 1 [9000/60000 (15%)] Loss: 2.009080 Accuracy: 56.70%\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 1.981732 Accuracy: 57.40%\n",
      "Train Epoch: 1 [11000/60000 (18%)] Loss: 1.928359 Accuracy: 62.40%\n",
      "Train Epoch: 1 [12000/60000 (20%)] Loss: 1.864750 Accuracy: 65.40%\n",
      "Train Epoch: 1 [13000/60000 (22%)] Loss: 1.797125 Accuracy: 65.90%\n",
      "Train Epoch: 1 [14000/60000 (23%)] Loss: 1.725841 Accuracy: 67.00%\n",
      "Train Epoch: 1 [15000/60000 (25%)] Loss: 1.672097 Accuracy: 64.80%\n",
      "Train Epoch: 1 [16000/60000 (27%)] Loss: 1.610159 Accuracy: 70.40%\n",
      "Train Epoch: 1 [17000/60000 (28%)] Loss: 1.552102 Accuracy: 71.80%\n",
      "Train Epoch: 1 [18000/60000 (30%)] Loss: 1.473842 Accuracy: 72.30%\n",
      "Train Epoch: 1 [19000/60000 (32%)] Loss: 1.445957 Accuracy: 73.10%\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 1.385425 Accuracy: 75.80%\n",
      "Train Epoch: 1 [21000/60000 (35%)] Loss: 1.322847 Accuracy: 73.80%\n",
      "Train Epoch: 1 [22000/60000 (37%)] Loss: 1.251562 Accuracy: 73.10%\n",
      "Train Epoch: 1 [23000/60000 (38%)] Loss: 1.216982 Accuracy: 75.40%\n",
      "Train Epoch: 1 [24000/60000 (40%)] Loss: 1.162111 Accuracy: 74.80%\n",
      "Train Epoch: 1 [25000/60000 (42%)] Loss: 1.102088 Accuracy: 79.00%\n",
      "Train Epoch: 1 [26000/60000 (43%)] Loss: 1.033968 Accuracy: 79.40%\n",
      "Train Epoch: 1 [27000/60000 (45%)] Loss: 1.048779 Accuracy: 77.50%\n",
      "Train Epoch: 1 [28000/60000 (47%)] Loss: 0.974437 Accuracy: 81.20%\n",
      "Train Epoch: 1 [29000/60000 (48%)] Loss: 0.953282 Accuracy: 79.80%\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.912417 Accuracy: 82.40%\n",
      "Train Epoch: 1 [31000/60000 (52%)] Loss: 0.896919 Accuracy: 80.10%\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.831962 Accuracy: 83.20%\n",
      "Train Epoch: 1 [33000/60000 (55%)] Loss: 0.817021 Accuracy: 83.50%\n",
      "Train Epoch: 1 [34000/60000 (57%)] Loss: 0.807760 Accuracy: 83.10%\n",
      "Train Epoch: 1 [35000/60000 (58%)] Loss: 0.810969 Accuracy: 81.80%\n",
      "Train Epoch: 1 [36000/60000 (60%)] Loss: 0.782640 Accuracy: 81.70%\n",
      "Train Epoch: 1 [37000/60000 (62%)] Loss: 0.774547 Accuracy: 80.40%\n",
      "Train Epoch: 1 [38000/60000 (63%)] Loss: 0.711445 Accuracy: 84.80%\n",
      "Train Epoch: 1 [39000/60000 (65%)] Loss: 0.703440 Accuracy: 84.90%\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.692485 Accuracy: 84.80%\n",
      "Train Epoch: 1 [41000/60000 (68%)] Loss: 0.689969 Accuracy: 85.00%\n",
      "Train Epoch: 1 [42000/60000 (70%)] Loss: 0.668208 Accuracy: 83.50%\n",
      "Train Epoch: 1 [43000/60000 (72%)] Loss: 0.708690 Accuracy: 82.90%\n",
      "Train Epoch: 1 [44000/60000 (73%)] Loss: 0.700420 Accuracy: 82.20%\n",
      "Train Epoch: 1 [45000/60000 (75%)] Loss: 0.632168 Accuracy: 85.60%\n",
      "Train Epoch: 1 [46000/60000 (77%)] Loss: 0.642322 Accuracy: 83.40%\n",
      "Train Epoch: 1 [47000/60000 (78%)] Loss: 0.622452 Accuracy: 85.00%\n",
      "Train Epoch: 1 [48000/60000 (80%)] Loss: 0.643565 Accuracy: 83.30%\n",
      "Train Epoch: 1 [49000/60000 (82%)] Loss: 0.609571 Accuracy: 85.00%\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.603569 Accuracy: 85.10%\n",
      "Train Epoch: 1 [51000/60000 (85%)] Loss: 0.562228 Accuracy: 86.80%\n",
      "Train Epoch: 1 [52000/60000 (87%)] Loss: 0.593789 Accuracy: 84.90%\n",
      "Train Epoch: 1 [53000/60000 (88%)] Loss: 0.529311 Accuracy: 87.10%\n",
      "Train Epoch: 1 [54000/60000 (90%)] Loss: 0.579474 Accuracy: 84.30%\n",
      "Train Epoch: 1 [55000/60000 (92%)] Loss: 0.566896 Accuracy: 85.40%\n",
      "Train Epoch: 1 [56000/60000 (93%)] Loss: 0.612302 Accuracy: 83.80%\n",
      "Train Epoch: 1 [57000/60000 (95%)] Loss: 0.557803 Accuracy: 84.90%\n",
      "Train Epoch: 1 [58000/60000 (97%)] Loss: 0.558203 Accuracy: 85.60%\n",
      "Train Epoch: 1 [59000/60000 (98%)] Loss: 0.551630 Accuracy: 85.10%\n",
      "\n",
      "Test set: Average loss: 0.5126, Accuracy: 8690/10000 (86.90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = NonSpikingNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "\n",
    "# 1 pass on the training data:\n",
    "train(model, device, train_set_loader, optimizer, epoch, logging_interval=1)\n",
    "test(model, device, test_set_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Well, I've trained just for 1 epoch here. But the results are about the same. Absolutely no tuning has been performed yet. I've coded this one-shot. It would be worth trying a few things to see how it goes. \n",
    "\n",
    "Using SNNs should act as a regularizer, just like dropout, as I wouldn't expect the neurons to fire all at the same time. Although, it's an interesting path to explore, as [Brain Rhythms](https://www.youtube.com/watch?v=OCpYdSN_kts) seems to play an important role in the brain, whereas in Deep Learning no such things happens. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
